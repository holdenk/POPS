language: python
python:
  - "2.7"
before_install:
  - sudo pip install --upgrade conda py4j
install:
  # install deps
  - sudo conda init
  - deps='pip numpy pandas requests nose numpydoc sphinx pep8 pylint scipy openpyxl'
  - conda create -p $HOME/py --yes $deps "python=$TRAVIS_PYTHON_VERSION"
  - export PATH=$HOME/py/bin:$PATH
  - pip install unittest2
  # build our scala code
  - ./sbt/sbt assembly
  # install our own package into the environment
  - "pip install ."
script:
  # Download spark 1.4.1
  - "wget http://d3kbcqa49mib13.cloudfront.net/spark-1.4.1-bin-hadoop2.4.tgz"
  - "tar -xf spark-1.4.1-bin-hadoop2.4.tgz"
  # Run the tests
  - "export SPARK_HOME=./spark-1.4.1-bin-hadoop2.4/"
  - "nosetests --logging-level=INFO --detailed-errors --verbosity=2 --with-coverage --cover-html-dir=./htmlcov --cover-package=sparklingpandas"
  - "pep8 --ignore=E402 sparklingpandas/"
  - "pylint -E sparklingpandas --extension-pkg-whitelist=numpy"
  - echo $PATH
  - export LOCATION=`pip show sparklingpandas|grep Location| cut -d ' ' -f 2`
  - echo $LOCATION
  - ls $LOCATION/../
  - ls $LOCATION/../../
  - ls $LOCATION/../../../
  - SHELL_LOCATION=$LOCATON/../../../
  - cd /tmp; echo "exit(0)" | sparklingpandashell
  